# UserSegments Factory

Сквозной ETL-пайплайн для формирования пользовательских сегментов и аналитических витрин. Проект имитирует рабочую среду с использованием Airflow для оркестрации, PySpark для обработки больших данных, PostgreSQL для хранения и кастомных инструментов для генерации синтетических данных и проверки их качества (Data Quality).

## Стек технологий

- **Apache Airflow** - оркестрация ETL-процессов и планирование задач
- **PySpark** - распределенная обработка данных и построение витрин
- **PostgreSQL** - хранение сырых данных и готовых витрин
- **Docker** - контейнеризация и управление сервисами
- **Python** - генерация данных, ETL-логика, проверка качества

## Архитектура и витрины

| Сервис | Сфера ответственности                                                                                                      |
|--------|----------------------------------------------------------------------------------------------------------------------------|
| **PostgreSQL** | • Инициализация таблиц<br> • Хранение сырых данных (users, orders, user_activity)<br>• Аналитические витрины (analytics.*) |
| **Airflow Orchestrator** | • Оркестрация задач<br>• Мониторинг выполнения                                                                             |
| **Data Generator** | • Создание синтетических данных<br>• Наполнение сырых таблиц                                                               |
| **Spark Cluster** | • Построение аналитических витрин<br>• Сегментация пользователей                                                           |
| **Data Quality Checker** | • Валидация данных<br>• Проверка целостности<br/>                                                                          |

Airflow Orchestrator (запуск каждые 2 минуты):
Data Generator → Spark ETL script → Data Quality Checker

### Аналитические витрины

В процессе ETL формируются:

#### 1. **user_segments** - пользовательские сегменты
```sql
user_id | segment_name | calculation_date
--------|--------------|-----------------
123     | vip          | 2024-01-15
456     | loyal        | 2024-01-15
789     | active       | 2024-01-15
```

Логика сегментации:

    vip - 10+ заказов
    loyal - 5+ заказов и сумма ≥500
    active - 3+ заказов
    new - регистрация в последние 30 дней
    churned - нет заказов 90+ дней
    regular - все остальные

#### 2. marketing_mart - расширенная маркетинговая витрина
```sql
user_id | segment_name | total_orders | total_income | last_activity_date | calculation_date
--------|--------------|--------------|---------------|-------------------|-----------------
123     | vip          | 15           | 2500.00       | 2024-01-14        | 2024-01-15
456     | loyal        | 7            | 800.50        | 2024-01-13        | 2024-01-15
```

## Быстрый старт
Для работы необходим установленный Docker.

```bash
git clone <repository-url>

# Создаем .env файл с настройками
make env

# Собираем образы
make build

# Запускаем всю инфраструктуру
make up
```

После выполнения `make up` последовательно запускается:

1. **Инициализация БД** - создание таблиц и схем
2. **Генерация тестовых данных** - наполнение сырых таблиц
3. **Запуск ETL-пайплайна** - выполнение DAG в Airflow
4. **Проверка качества данных** - валидация результатов ETL

Первый запуск занимает вплоть до 10 минут.\
Исполнение DAG занимает до минуты, после чего можно проверить таблицы. \
Команда `make psql` дает доступ к БД.

Сегменты пользователей: `SELECT * FROM analytics.user_segments;`\
Расширенная маркетинговая витрина: `SELECT * FROM analytics.marketing_mart;`\
Статистика по сегментам:
```sql
SELECT segment_name, COUNT(*) as users_count 
FROM analytics.user_segments 
GROUP BY segment_name;
```

## Доступ к сервисам

- **Airflow UI**: http://localhost:8081
- **Spark**: http://localhost:8085
- `make psql` для просмотра БД через утилиту psql

## Ключевые особенности

- **Полная автоматизация** - сквозной пайплайн от генерации данных до проверки качества
- **Промышленный стек** - технологии, используемые в реальных Data Engineering проектах
- **Модульность** - каждый сервис независим и легко модифицируется
- **Data Quality** - встроенные проверки целостности и качества данных
- **Готовность к масштабированию** - Spark-обработка легко масштабируется на большие объемы данных

## Направления развития

- Мониторинг и алертинг (Grafana + Prometheus)
- Добавление новых источников данных
- Интеграция с Kafka для потоковой обработки
- Расширение проверок Data Quality

